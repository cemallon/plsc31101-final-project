Is Asylum International Biased?
========================================================
author: Casey Mallon
date: 12/10/19
autosize: true

Why does it matter? 
========================================================
Recent migration crises.<br />
Recent conservative political figures questioning
the authority of NGOs

Data Collection
========================================================
```{r,echo=F}
require(tidyverse)
require(dplyr)
require(rvest)
require(stringr)
require(purrr)
require(lubridate)
```
```{r}
base<- "https://www.amnesty.org/en/latest/news/?contentType=2561&issue=1613&sort=date&p="
page=c(1:11)%>%as.character()
webpages<- str_c(base, page)

#gets all the URLs from the search results
all_URLs<- function(i){
  URLs<- read_html(i)%>%
    html_nodes(".search-item__link")%>%
    html_attr("href")%>%
    as.character()
  results<- paste0("https://www.amnesty.org", URLs)
  return(results)
}
#all of the news articles
ALL_URLS<- map(webpages, all_URLs)%>%
  unlist()

#function to get the region, title, date, and text of each article
scrape_text <- function(url){
  webpage<- read_html(url)
  text <- html_nodes(webpage, ".wysiwyg") %>%
    html_text()
  title<- html_nodes(webpage,".heading--main")%>%
    html_text()
  region<-html_nodes(webpage, ".tags__item--bold--sm+ .tags__item--discrete .tags__link--discrete--md")%>%
    html_text()
  date<- html_nodes(webpage, "time")%>%
    html_text()
  all_info<- data.frame(date=ifelse(length(date)==0, NA, date),
                        region=ifelse(length(region)==0,NA, region),
                        title= ifelse(length(title)==0,NA,title),
                        text= ifelse(length(text)==0,NA,text))
  return(all_info)
}
#Need to include the ifelse part in the data.frame in case articles aren't formatted in the same way
#Apparently its too much money to have uniform structure
#some of the articles aren't actually formatted using the same nodes, 
#some titles use nodes that mean different things on different pages

#Making a giant data frame
Amenesty_International_Asylum<- map_dfr(ALL_URLS,scrape_text)
```

Tidying the Data
========================================================
```{r,echo=F}
require(tidyverse)
require(dplyr)
require(rvest)
require(stringr)
require(purrr)
require(lubridate)
Asylum<- read.csv(file="/Users/cemallon/Documents/Computational-Tools/plsc31101-final-project/Data/Amenesty_International_Asylum.csv",stringsAsFactors = FALSE)
```
```{r}
#filter out all the NAs
Asylum_Full<- Asylum%>%
  filter(!is.na(region), !is.na(date), !is.na(text), !is.na(title))%>%
  subset(select=-c(X))

#filter out all the incorrect titles
#these are titles like "Amnesty's Experts"
#the reason is that these pages are coded differently than 
Asylum_Title<-Asylum_Full%>%
  filter(title!="Amnesty's experts")
#number of observations dropped from 159 to 132

#This gives everything a nice name and simplifies the date to just the year
Asylum_Tidy<-Asylum_Title%>%
  rename(Date=date, Region=region, Title=title, Text=text)%>%
  mutate(Date=as.numeric(str_extract(Date,"\\d{4}")))


#Next I need to manually collapse the regions
#For some reason, Amnesty International doesn't stick to regions
#In a few cases they list specific countries
#I'll continue to keep the Turkey, Afghanistan, and Russia left as separate entities
#Miscoding on the original site for two articles, marked the regions as Refugees and Detention
#Recoded to Europe

Region_clean<- recode(Asylum_Tidy$Region,"Burundi"="Africa", "Congo"="Africa", "Kenya"="Africa", "Eritrea"="Africa",
                         "Libya"="Middle East and North Africa","Middle East and North Africa " = "Middle East and North Africa",
                         "Hungary"= "Europe and Central Asia", "Spain"="Europe and Central Asia", 
                         "France"="Europe and Central Asia","Greece"="Europe and Central Asia","Italy"="Europe and Central Asia",
                         "Ecuador"="Americas","Venezuela"="Americas", "Cuba"="Americas",
                         "Nauru" = "Asia and The Pacific", "Australia"="Asia and The Pacific", "Malaysia"="Asia and The Pacific",
                         "Refugees"="Europe and Central Asia","Detention"="Europe and Central Asia",
                          "United States of America"="Americas")

Asylum_Tidy$Region_clean<- as.character(Region_clean)

#Removing the old Region 
Asylum_Tidyer<- Asylum_Tidy%>%
  subset(select= -c(Region))%>%
  rename("Region"="Region_clean")%>%
 subset(select=c(1,4,2,3))
```

Text Analysis
========================================================
```{r,echo=F}
## Getting Set up
require(tm)
require(ggplot2)
require (wordcloud)
require(tidytext)
require(tidyverse)
require(textdata)
require(dplyr)
require(glmnet)
require(stargazer)
require(gridExtra)
Asylum.dat<- read.csv(file="/Users/cemallon/Documents/Computational-Tools/plsc31101-final-project/Data/Amenesty_International_Asylum_CLEAN.csv",stringsAsFactors = FALSE)
```
```{r}
##Pre-Processing
Asylum<-Corpus(VectorSource(Asylum.dat$Text))
Asylum_dtm <- DocumentTermMatrix(Asylum,
                          control = list(stopwords = TRUE,
                                         tolower = TRUE,
                                         removeNumbers = TRUE,
                                         removePunctuation = TRUE))
Asylum_dtm<-as.data.frame(as.matrix(Asylum_dtm))

#Using bing lexicon for simple binary positive v. negative categorization
Asylum_sentiments<- get_sentiments("bing")

Asylum_sentiments$score<-ifelse(Asylum_sentiments$sentiment=="positive",1,-1)


#Score each article 
Words<- data.frame(word=colnames(Asylum_dtm))
Words<-merge(Words,Asylum_sentiments,all.x=T)
Words$score[is.na(Words$score)]<-0
Scores<- as.matrix(Asylum_dtm)%*%Words$score

##Edit the dataframe
Asylum.dat$Scores= Scores
Asylum.dat<- Asylum.dat%>%
  subset(select=-c(X))
``` 

Which words are the most frequent?
========================================================
```{r,echo=F}
Asylum_frequency<-colSums(as.matrix(Asylum_dtm))
Asylum_sorted<-sort(Asylum_frequency,decreasing=F)
set.seed(478)
wordcloud(names(Asylum_sorted),Asylum_sorted,max.words=50, colors=brewer.pal(4,"Paired"),scale=c(2.0,0.25),res=300)
```

Average Sentiment Scores Over Time
========================================================
```{r,echo=F}
Asylum.dat%>%
  group_by(Region,Date)%>%
  summarise(Mean_Scores=mean(Scores, na.rm=T))%>%
  ggplot(aes(x=Date, y=Mean_Scores,color=Region))+
  geom_line(size=1.5)+
  ylab("Mean Sentiment Score")+
  xlab("Year")+
  ggtitle("Average Sentiment Scores Over Time")
```

Average Sentiment Scores by Region
========================================================
```{r,echo=F}
Asylum.dat%>%
  group_by(Date,Region)%>%
  summarise(MScore=mean(Scores, na.rm=T))%>%
  as_tibble()%>%
  ggplot(aes(x=Region,MScore, y=MScore,fill=Region))+
  geom_col()+
  scale_color_brewer("Paired")+
  ylab("Sentiment Score")+
  xlab("Region")+
  ggtitle("Average Sentiment Scores By Region")+
  scale_y_reverse()+
  theme(axis.text.x=element_text(angle=45,vjust=1,hjust=1))
```

Average Sentiments Scores by Region per Year
========================================================
```{r,echo=F}
##Column Plot 2015
Asylumyear1<- Asylum.dat%>%
  group_by(Region)%>%
  filter(Date=="2015")%>%
  summarise(MScore=mean(Scores, na.rm=T))%>%
  as_tibble()%>%
  ggplot(aes(x=Region,MScore, y=MScore,fill=Region))+
  geom_col()+
  scale_color_brewer("Paired")+
  ylab("Sentiment Score")+
  xlab("Region")+
  ggtitle("2015 Average Sentiment Scores By Region")+
  scale_y_reverse()+
  theme(axis.text.x=element_text(angle=45,vjust=1,hjust=1))

#Column Plot 2016
Asylumyear2<- Asylum.dat%>%
  group_by(Region)%>%
  filter(Date=="2016")%>%
  summarise(MScore=mean(Scores, na.rm=T))%>%
  as_tibble()%>%
  ggplot(aes(x=Region,MScore, y=MScore,fill=Region))+
  geom_col()+
  scale_color_brewer("Paired")+
  ylab("Sentiment Score")+
  xlab("Region")+
  ggtitle("2016 Average Sentiment Scores By Region")+
  scale_y_reverse()+
  theme(axis.text.x=element_text(angle=45,vjust=1,hjust=1))

#Column Plot 2017
Asylumyear3<- Asylum.dat%>%
  group_by(Region)%>%
  filter(Date=="2017")%>%
  summarise(MScore=mean(Scores, na.rm=T))%>%
  as_tibble()%>%
  ggplot(aes(x=Region,MScore, y=MScore,fill=Region))+
  geom_col()+
  scale_color_brewer("Paired")+
  ylab("Sentiment Score")+
  xlab("Region")+
  ggtitle("2017 Average Sentiment Scores By Region")+
  scale_y_reverse()+
  theme(axis.text.x=element_text(angle=45,vjust=1,hjust=1))

#Column Plot 2018
Asylumyear4<- Asylum.dat%>%
  group_by(Region)%>%
  filter(Date=="2018")%>%
  summarise(MScore=mean(Scores, na.rm=T))%>%
  as_tibble()%>%
  ggplot(aes(x=Region,MScore, y=MScore,fill=Region))+
  geom_col()+
  scale_color_brewer("Paired")+
  ylab("Sentiment Score")+
  xlab("Region")+
  ggtitle("2018 Average Sentiment Scores By Region")+
  scale_y_reverse()+
  theme(axis.text.x=element_text(angle=45,vjust=1,hjust=1))

#Column Plot 2019
Asylumyear5<- Asylum.dat%>%
  group_by(Region)%>%
  filter(Date=="2019")%>%
  summarise(MScore=mean(Scores, na.rm=T))%>%
  as_tibble()%>%
  ggplot(aes(x=Region,MScore, y=MScore,fill=Region))+
  geom_col()+
  scale_color_brewer("Paired")+
  ylab("Sentiment Score")+
  xlab("Region")+
  ggtitle("2019 Average Sentiment Scores By Region")+
  scale_y_reverse()+
  theme(axis.text.x=element_text(angle=45,vjust=1,hjust=1))

grid.arrange(Asylumyear1, Asylumyear2, Asylumyear3, Asylumyear4, Asylumyear5, nrow=2)
```

